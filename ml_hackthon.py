# -*- coding: utf-8 -*-
"""ML_Hackthon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Im4kHTNDoyN1ioeCD7TCmtdxh-odtCbM
"""

!pip install google-cloud-vision
import pandas as pd
import numpy as np
import requests
import os
import re
from google.cloud import vision
import io

df1=pd.read_csv('train.csv')
df1.head()

df1.shape[0]

df=pd.read_csv('test.csv')
df.head()

print(df1['group_id'].nunique())
print(df['group_id'].nunique())

# def extract_value_and_unit(entity_value):
#   """Extracts value and unit from the entity_value string."""
#   try:
#     parts = entity_value.split()
#     if len(parts) > 1:
#       value = parts[0]
#       unit = parts[-1]
#       return value, unit
#     else:
#       return entity_value, None  # If no unit is present, return the whole string as value
#   except:
#     return None, None


# # Apply the function to the 'entity_value' column and create new columns
# df2=df[['value', 'unit']] = df1['entity_value'].apply(extract_value_and_unit).apply(pd.Series)

# df2.head()

num_rows = df.shape[0]
print(num_rows)

import os
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "key.json"

client = vision.ImageAnnotatorClient()

def detect_text_from_url(image_url):
    """Detects text in an image URL using Google Cloud Vision API"""
    image = vision.Image()
    image.source.image_uri = image_url

    response = client.text_detection(image=image)
    texts = response.text_annotations

    if response.error.message:
        raise Exception(f'Error: {response.error.message}')

    return texts[0].description if texts else None

import re

# Function to search the text for an entity and its units, with compulsory output
def prediction(entity_name, text):
    # Define the unit mappings for different entities
    item_weight = {
        "mcg": "microgram",
        "g": "gram",
        "kg": "kilogram",
        "oz": "ounce",
        "lb": "pound",
        "ton": "ton"
    }

    maximum_weight_recommendation = {
        "mcg": "microgram",
        "g": "gram",
        "kg": "kilogram",
        "oz": "ounce",
        "lb": "pound",
        "ton": "ton"
    }

    width = {
        "mm": "millimetre",
        "cm": "centimetre",
        "in": "inch",
        "foot": "foot",
        "yard": "yard",
        "m": "metre"
    }

    depth = {
        "mm": "millimetre",
        "cm": "centimetre",
        "in": "inch",
        "foot": "foot",
        "yard": "yard",
        "m": "metre"
    }

    height = {
        "mm": "millimetre",
        "cm": "centimetre",
        "in": "inch",
        "foot": "foot",
        "yard": "yard",
        "m": "metre"
    }

    wattage = {
        "w": "watt",
        "kwh": "kilowatt"
    }

    voltage = {
        "mv": "millivolt",
        "v": "volt",
        "kv": "kilovolt"
    }

    item_volume = {
        "Âµl": "microlitre",
        "ml": "millilitre",
        "cl": "centilitre",
        "dl": "decilitre",
        "cup": "cup",
        "fl oz": "fluid ounce",
        "pt": "pint",
        "qt": "quart",
        "l": "litre",
        "gal": "gallon",
        "imp gal": "imperial gallon",
        "cu in": "cubic inch",
        "cu ft": "cubic foot"
    }

    # Dictionary to map entity names to their corresponding units dictionaries
    units_dict = {
        "width": width,
        "depth": depth,
        "height": height,
        "item_weight": item_weight,
        "maximum_weight_recommendation": maximum_weight_recommendation,
        "voltage": voltage,
        "wattage": wattage,
        "item_volume": item_volume
    }

    # Default units for each entity
    default_units = {
        "height": "centimetre",
        "width": "centimetre",
        "depth": "centimetre",
        "item_weight": "gram",
        "maximum_weight_recommendation": "gram",
        "wattage": "watt",
        "voltage": "volt",
        "item_volume": "millilitre"
    }

    # Get the corresponding dictionary for the entity
    entity_units = units_dict.get(entity_name)


    # List to store all valid results found
    results = []
    unit_name = None

    # Iterate through each unit key in the dictionary
    for unit_key, unit_name in entity_units.items():
        # Create a regex pattern to find value followed by the key (e.g., 9.5cm, 46mm)
        pattern = r"(\d+\.?\d*)\s*" + unit_key

        # Find matches for this particular unit key
        matches = re.findall(pattern, text, re.IGNORECASE)

        if matches:
            for match in matches:
                value = match.replace(',', '.')  # Replace commas with periods for decimals
                results.append(float(value))  # Store value and unit
            break  # Stop after finding the first matching unit

    # If results are found, determine the best value based on the entity type
    if results:
        best_result = 1.0
        if entity_name in ["height", "depth", "item_weight", "maximum_weight_recommendation", "wattage"]:  # Use max value for these entities
            best_result = max(results)
        elif entity_name in ["width", "voltage"]:  # Use min value for these entities
            best_result = min(results)

        return f"{best_result} {unit_name}"

    # If no units were found, try to find numbers without units
    numbers_without_units = re.findall(r"(\d+\.?\d*)", text)
    if numbers_without_units:
        best_value = 1.0
        numbers = [float(num.replace(',', '.')) for num in numbers_without_units]

        if entity_name in ["height", "depth", "item_weight", "maximum_weight_recommendation", "wattage"]:
            best_value = max(numbers)  # Use max number for these entities
        elif entity_name in ["width", "voltage"]:
            best_value = min(numbers)  # Use min number for these entities

        # Return best value with default unit for the entity
        return f"{best_value} {default_units[entity_name]}"

    # If no values are found, return a default value
    return f"1.0 {default_units[entity_name]}"

batch_results = []
image_url = 'https://m.media-amazon.com/images/I/21DZ7BAZ6-L.jpg'
text = detect_text_from_url(image_url)
if text:
    text = text.lower()
else:
    text = "xxxx"

# Use the prediction function to get the result
prediction_result = prediction("item_volume", text)

# Append the index and prediction to the batch results
batch_results.append([1, prediction_result])
print(batch_results)

import pandas as pd

# Function to split DataFrame into chunks of 10,000
def split_dataframe(df, chunk_size=1000):
    for i in range(0, len(df), chunk_size):
        yield df[i:i + chunk_size]

# Create an empty list to store the results
results = []

# Assuming df is your DataFrame with 120,000 rows and 'image_link' and 'entity_name' columns
batch_size = 1000# Size of each batch
batch_count = 0     # Counter for batches

# Iterate through the DataFrame in chunks of 10,000
for batch_df in split_dataframe(df, batch_size):
    batch_results = []  # Store results for the current batch

    # Process each row in the current batch
    for _, row in batch_df.iterrows():
        try:

            image_url = str(row['image_link'])
            text = detect_text_from_url(image_url)
            if text:
                text = text.lower()
            else:
                text = "xxxx"

            # Use the prediction function to get the result
            prediction_result = prediction(row["entity_name"], text)

            # Append the index and prediction to the batch results
            batch_results.append([row['index'], prediction_result])

        except Exception as e:
            # Handle exceptions such as network errors or text extraction issues
            print(f"Error processing image {row['image_link']}: {e}")
            batch_results.append([row['index'], ""])  # Append empty result for this row

    # Convert the batch results into a DataFrame
    batch_df_out = pd.DataFrame(batch_results, columns=['index', 'prediction'])

    # If it's the first batch, write a new CSV file, otherwise append to the existing file
    if batch_count == 0:
        batch_df_out.to_csv('test_out.csv', index=False, mode='w', header=True)
    else:
        batch_df_out.to_csv('test_out.csv', index=False, mode='a', header=False)

    # Increment the batch counter
    batch_count += 1
    print(f"Batch {batch_count} processed and appended to CSV.")




print("All batches processed and saved to test_out.csv")

